{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "  # %tensorflow_version only exists in Colab.\n",
    "  %tensorflow_version 2.x\n",
    "except Exception:\n",
    "  pass\n",
    "import tensorflow as tf\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import re\n",
    "import numpy as np\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"/kaggle/input/sentiment140/training.1600000.processed.noemoticon.csv\", encoding='latin-1', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3                4  \\\n",
       "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                   5  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments=df[0].values\n",
    "sentences=df[5].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments[sentiments==4]=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " \"is upset that he can't update his Facebook by texting it... and might cry as a result  School today also. Blah!\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiments[1],sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "total=len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "split=int(0.95*total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1520000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentiments=sentiments[split:]\n",
    "test_sentences=sentences[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preprocess(sentence,sentiment):\n",
    "# #     sentence = tf.strings.lower(sentence)\n",
    "# #     sentence = tf.strings.regex_replace(sentence,'[^a-zA-Z0-9?.!,\\'\\\"]',' ')\n",
    "# #     words_list = tf.strings.split(sentence)\n",
    "# #     sentence = tf.strings.reduce_join(words_list,separator=\" \", axis=-1)\n",
    "#     sentiment = tf.one_hot(sentiment, 2)\n",
    "    \n",
    "#     return sentence, sentiment\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=2000\n",
    "train = tf.data.Dataset.from_tensor_slices((tf.constant(sentences),tf.constant(sentiments)))\n",
    "# train = train.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "train = train.cache().shuffle(total)\n",
    "train = train.batch(batch_size).prefetch(buffer_size=tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=string, numpy=b'@iFrankM my sister and I were about to head over to Yard House but they have 0 free tables '>,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch, example_target_batch = next(iter(train))\n",
    "example_input_batch[-1], example_target_batch[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(), dtype=string, numpy=b\"@mjh81 lols  didn't wanna watch it really . . . Lmao!\">,\n",
       " <tf.Tensor: shape=(), dtype=int64, numpy=1>)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch[0], example_target_batch[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=1000\n",
    "test = tf.data.Dataset.from_tensor_slices((tf.constant(test_sentences),tf.constant(test_sentiments)))\n",
    "# test = test.map(preprocess, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "test = test.cache().shuffle(len(test_sentences))\n",
    "test = test.batch(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "hub_layer = hub.KerasLayer(\"https://tfhub.dev/google/tf2-preview/nnlm-en-dim128-with-normalization/1\", output_shape=[128],\n",
    "                           input_shape=[], dtype=tf.string, trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "model.add(hub_layer)\n",
    "model.add(tf.keras.layers.Lambda(lambda x: tf.expand_dims(x,1)))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(160,return_sequences=True)))\n",
    "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(160)))\n",
    "model.add(tf.keras.layers.Dense(160, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(30, activation='relu'))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "keras_layer (KerasLayer)     (None, 128)               124642688 \n",
      "_________________________________________________________________\n",
      "lambda (Lambda)              (None, 1, 128)            0         \n",
      "_________________________________________________________________\n",
      "bidirectional (Bidirectional (None, 1, 320)            369920    \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 320)               615680    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 160)               51360     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 30)                4830      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 30)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 62        \n",
      "=================================================================\n",
      "Total params: 125,684,540\n",
      "Trainable params: 125,684,540\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.optimizers.Adam(learning_rate=0.001), \n",
    "              loss = 'sparse_categorical_crossentropy', \n",
    "              metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import datetime\n",
    "\n",
    "filepath=\"cp/sentiment_model.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "# callbacks_list = [checkpoint]\n",
    "!mkdir cp\n",
    "\n",
    "\n",
    "\n",
    "# log_dir='logs/fit/'+ datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "\n",
    "# tb=tf.keras.callbacks.TensorBoard(log_dir=log_dir,histogram_freq=1)\n",
    "\n",
    "callbacks_list = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir logs\n",
    "# !mkdir logs/fit/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorboard --logdir logs/fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 800 steps, validate for 80 steps\n",
      "Epoch 1/120\n",
      "799/800 [============================>.] - ETA: 0s - loss: 0.4435 - accuracy: 0.7940\n",
      "Epoch 00001: val_loss improved from inf to 0.40717, saving model to cp/sentiment_model.hdf5\n",
      "800/800 [==============================] - 61s 76ms/step - loss: 0.4435 - accuracy: 0.7940 - val_loss: 0.4072 - val_accuracy: 0.7922\n",
      "Epoch 2/120\n",
      "799/800 [============================>.] - ETA: 0s - loss: 0.3987 - accuracy: 0.8180\n",
      "Epoch 00002: val_loss improved from 0.40717 to 0.39342, saving model to cp/sentiment_model.hdf5\n",
      "800/800 [==============================] - 52s 65ms/step - loss: 0.3987 - accuracy: 0.8180 - val_loss: 0.3934 - val_accuracy: 0.8027\n",
      "Epoch 3/120\n",
      "799/800 [============================>.] - ETA: 0s - loss: 0.3699 - accuracy: 0.8328\n",
      "Epoch 00003: val_loss improved from 0.39342 to 0.35056, saving model to cp/sentiment_model.hdf5\n",
      "800/800 [==============================] - 52s 65ms/step - loss: 0.3699 - accuracy: 0.8328 - val_loss: 0.3506 - val_accuracy: 0.8368\n",
      "Epoch 4/120\n",
      "799/800 [============================>.] - ETA: 0s - loss: 0.3396 - accuracy: 0.8484\n",
      "Epoch 00004: val_loss improved from 0.35056 to 0.32087, saving model to cp/sentiment_model.hdf5\n",
      "800/800 [==============================] - 51s 64ms/step - loss: 0.3396 - accuracy: 0.8484 - val_loss: 0.3209 - val_accuracy: 0.8509\n",
      "Epoch 5/120\n",
      "799/800 [============================>.] - ETA: 0s - loss: 0.3090 - accuracy: 0.8639\n",
      "Epoch 00005: val_loss improved from 0.32087 to 0.27086, saving model to cp/sentiment_model.hdf5\n",
      "800/800 [==============================] - 51s 64ms/step - loss: 0.3091 - accuracy: 0.8639 - val_loss: 0.2709 - val_accuracy: 0.8786\n",
      "Epoch 6/120\n",
      "799/800 [============================>.] - ETA: 0s - loss: 0.2797 - accuracy: 0.8782\n",
      "Epoch 00006: val_loss improved from 0.27086 to 0.26768, saving model to cp/sentiment_model.hdf5\n",
      "800/800 [==============================] - 50s 63ms/step - loss: 0.2797 - accuracy: 0.8782 - val_loss: 0.2677 - val_accuracy: 0.8767\n",
      "Epoch 7/120\n",
      "799/800 [============================>.] - ETA: 0s - loss: 0.2066 - accuracy: 0.9120\n",
      "Epoch 00009: val_loss improved from 0.21142 to 0.20814, saving model to cp/sentiment_model.hdf5\n",
      "800/800 [==============================] - 50s 63ms/step - loss: 0.2066 - accuracy: 0.9120 - val_loss: 0.2081 - val_accuracy: 0.9032\n",
      "Epoch 10/120\n",
      "799/800 [============================>.] - ETA: 0s - loss: 0.1881 - accuracy: 0.9205\n",
      "Epoch 00010: val_loss improved from 0.20814 to 0.17308, saving model to cp/sentiment_model.hdf5\n",
      "800/800 [==============================] - 56s 70ms/step - loss: 0.1881 - accuracy: 0.9205 - val_loss: 0.1731 - val_accuracy: 0.9208\n",
      "Epoch 11/120\n",
      "798/800 [============================>.] - ETA: 0s - loss: 0.1722 - accuracy: 0.9275\n",
      "Epoch 00011: val_loss improved from 0.17308 to 0.16038, saving model to cp/sentiment_model.hdf5\n",
      "800/800 [==============================] - 50s 62ms/step - loss: 0.1722 - accuracy: 0.9275 - val_loss: 0.1604 - val_accuracy: 0.9263\n",
      "Epoch 12/120\n",
      "799/800 [============================>.] - ETA: 0s - loss: 0.1580 - accuracy: 0.9337\n",
      "Epoch 00012: val_loss improved from 0.16038 to 0.15874, saving model to cp/sentiment_model.hdf5\n",
      "800/800 [==============================] - 54s 68ms/step - loss: 0.1580 - accuracy: 0.9337 - val_loss: 0.1587 - val_accuracy: 0.9293\n",
      "Epoch 13/120\n",
      "798/800 [============================>.] - ETA: 0s - loss: 0.1458 - accuracy: 0.9390\n",
      "Epoch 00013: val_loss improved from 0.15874 to 0.13534, saving model to cp/sentiment_model.hdf5\n",
      "800/800 [==============================] - 49s 61ms/step - loss: 0.1458 - accuracy: 0.9390 - val_loss: 0.1353 - val_accuracy: 0.9377\n",
      "Epoch 14/120\n",
      "798/800 [============================>.] - ETA: 0s - loss: 0.1348 - accuracy: 0.9436\n",
      "Epoch 00014: val_loss improved from 0.13534 to 0.12695, saving model to cp/sentiment_model.hdf5\n",
      "800/800 [==============================] - 50s 63ms/step - loss: 0.1348 - accuracy: 0.9436 - val_loss: 0.1269 - val_accuracy: 0.9419\n",
      "Epoch 15/120\n",
      "799/800 [============================>.] - ETA: 0s - loss: 0.1249 - accuracy: 0.9478\n",
      "Epoch 00015: val_loss improved from 0.12695 to 0.11472, saving model to cp/sentiment_model.hdf5\n",
      "800/800 [==============================] - 51s 64ms/step - loss: 0.1249 - accuracy: 0.9478 - val_loss: 0.1147 - val_accuracy: 0.9477\n",
      "Epoch 16/120\n",
      "799/800 [============================>.] - ETA: 0s - loss: 0.1159 - accuracy: 0.9516\n",
      "Epoch 00016: val_loss did not improve from 0.11472\n",
      "800/800 [==============================] - 48s 60ms/step - loss: 0.1159 - accuracy: 0.9516 - val_loss: 0.1172 - val_accuracy: 0.9448\n",
      "Epoch 17/120\n",
      "799/800 [============================>.] - ETA: 0s - loss: 0.1079 - accuracy: 0.9548\n",
      "Epoch 00017: val_loss improved from 0.11472 to 0.09546, saving model to cp/sentiment_model.hdf5\n",
      "800/800 [==============================] - 51s 63ms/step - loss: 0.1079 - accuracy: 0.9548 - val_loss: 0.0955 - val_accuracy: 0.9548\n",
      "Epoch 18/120\n",
      "799/800 [============================>.] - ETA: 0s - loss: 0.1003 - accuracy: 0.9579\n",
      "Epoch 00018: val_loss did not improve from 0.09546\n",
      "800/800 [==============================] - 48s 60ms/step - loss: 0.1003 - accuracy: 0.9579 - val_loss: 0.0959 - val_accuracy: 0.9583\n",
      "Epoch 19/120\n",
      "799/800 [============================>.] - ETA: 0s - loss: 0.0935 - accuracy: 0.9608\n",
      "Epoch 00019: val_loss improved from 0.09546 to 0.09358, saving model to cp/sentiment_model.hdf5\n",
      "800/800 [==============================] - 50s 63ms/step - loss: 0.0935 - accuracy: 0.9607 - val_loss: 0.0936 - val_accuracy: 0.9582\n",
      "Epoch 20/120\n",
      "800/800 [==============================] - 49s 62ms/step - loss: 0.0114 - accuracy: 0.9952 - val_loss: 0.0086 - val_accuracy: 0.9947\n",
      "Epoch 70/120\n",
      "798/800 [============================>.] - ETA: 0s - loss: 0.0095 - accuracy: 0.9959\n",
      "Epoch 00080: val_loss did not improve from 0.00782\n",
      "800/800 [==============================] - 46s 58ms/step - loss: 0.0095 - accuracy: 0.9959 - val_loss: 0.0080 - val_accuracy: 0.9951\n",
      "Epoch 81/120\n",
      "798/800 [============================>.] - ETA: 0s - loss: 0.0084 - accuracy: 0.9963\n",
      "Epoch 00090: val_loss did not improve from 0.00695\n",
      "800/800 [==============================] - 52s 65ms/step - loss: 0.0084 - accuracy: 0.9963 - val_loss: 0.0073 - val_accuracy: 0.9955\n",
      "Epoch 91/120\n",
      "798/800 [============================>.] - ETA: 0s - loss: 0.0076 - accuracy: 0.9965\n",
      "Epoch 00101: val_loss did not improve from 0.00604\n",
      "800/800 [==============================] - 45s 57ms/step - loss: 0.0076 - accuracy: 0.9965 - val_loss: 0.0075 - val_accuracy: 0.9953\n",
      "Epoch 102/120\n",
      "798/800 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9966\n",
      "Epoch 00102: val_loss did not improve from 0.00604\n",
      "800/800 [==============================] - 45s 56ms/step - loss: 0.0075 - accuracy: 0.9966 - val_loss: 0.0070 - val_accuracy: 0.9957\n",
      "Epoch 103/120\n",
      "798/800 [============================>.] - ETA: 0s - loss: 0.0075 - accuracy: 0.9966\n",
      "Epoch 00103: val_loss did not improve from 0.00604\n",
      "800/800 [==============================] - 51s 64ms/step - loss: 0.0075 - accuracy: 0.9966 - val_loss: 0.0063 - val_accuracy: 0.9957\n",
      "Epoch 104/120\n",
      "799/800 [============================>.] - ETA: 0s - loss: 0.0073 - accuracy: 0.9966\n",
      "Epoch 00104: val_loss improved from 0.00604 to 0.00602, saving model to cp/sentiment_model.hdf5\n",
      "800/800 [==============================] - 49s 61ms/step - loss: 0.0073 - accuracy: 0.9966 - val_loss: 0.0060 - val_accuracy: 0.9959\n",
      "Epoch 105/120\n",
      "533/800 [==================>...........] - ETA: 15s - loss: 0.0066 - accuracy: 0.9969"
     ]
    }
   ],
   "source": [
    "history = model.fit(train, epochs=120,validation_data=test, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(\"cp/sentiment_model.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.image  as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#-----------------------------------------------------------\n",
    "# Retrieve a list of list results on training and test data\n",
    "# sets for each training epoch\n",
    "#-----------------------------------------------------------\n",
    "loss=history.history['loss']\n",
    "v_loss=history.history['val_loss']\n",
    "\n",
    "\n",
    "epochs=range(len(loss)) # Get number of epochs\n",
    "\n",
    "\n",
    "#------------------------------------------------\n",
    "# Plot training and validation loss per epoch\n",
    "#------------------------------------------------\n",
    "plt.plot(epochs, loss, 'r')\n",
    "plt.plot(epochs, v_loss, 'b')\n",
    "plt.title('loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Loss\"])\n",
    "\n",
    "plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(text):\n",
    "    start_at = time.time()\n",
    "    \n",
    "    score = model.predict([text])[0]\n",
    "\n",
    "    return {\"negative\": float(score[0]), \"postive\": float(score[1]),\n",
    "       \"elapsed_time\": time.time()-start_at}  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 1.0,\n",
       " 'postive': 1.2868395585230452e-12,\n",
       " 'elapsed_time': 1.1083214282989502}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"the product made my skin glow but it had severe side effects\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'negative': 4.458362845571884e-11,\n",
       " 'postive': 1.0,\n",
       " 'elapsed_time': 0.031017780303955078}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(\"Blackpink is the greatest girl band ever.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
